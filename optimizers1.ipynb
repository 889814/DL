{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87decc6b-75d7-4bda-9b6b-422b585e96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.randn(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "#Add bias term\\\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Batch Gradient Descent\n",
    "\n",
    "#Batch Gradient Descent\n",
    "def batch_gradient_descent(X, y, alpha = 0.1, num_iters = 1000):\n",
    "  m = len(y)\n",
    "  theta = np.random.randn(2,1)\n",
    "  print(theta)\n",
    "  for iteration in range(num_iters):\n",
    "    gradient = 2/m * X.T.dot(X.dot(theta) - y)\n",
    "    theta = theta - alpha * gradient\n",
    "  return theta\n",
    "\n",
    "theta_bgd = batch_gradient_descent(X_b, y)\n",
    "print(\"BATCH GRADIENT DESCENT THETA\",theta_bgd)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "def stochastic_gradient_descent(X, y, alpha = 0.1, n_epochs = 50):\n",
    "  m = len(y)\n",
    "  theta = np.random.randn(2,1)\n",
    "  for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "      random_index = np.random.randint(m)\n",
    "      xi = X[random_index:random_index+1]\n",
    "      yi = y[random_index:random_index+1]\n",
    "      gradient = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "      theta = theta - alpha * gradient\n",
    "  return theta\n",
    "\n",
    "theta_sgd = stochastic_gradient_descent(X_b, y)\n",
    "print(\"STOCHASTIC GRADIENT DESCENT THETA\",theta_sgd)\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "def mini_batch_gradient_descent(X, y, alpha = 0.1, num_iters = 1000, batch_size = 20):\n",
    "  m = len(y)\n",
    "  theta = np.random.randn(2,1)\n",
    "  for iteration in range(num_iters):\n",
    "    indices = np.random.permutation(m)\n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    for i in range(0, m, batch_size):\n",
    "      xi = X_shuffled[i:i+batch_size]\n",
    "      yi = y_shuffled[i:i+batch_size]\n",
    "      gradient = 2/len(xi) * xi.T.dot(xi.dot(theta) - yi)\n",
    "      theta = theta - alpha * gradient\n",
    "    return theta\n",
    "\n",
    "theta_mbgd = mini_batch_gradient_descent(X_b, y)\n",
    "print(\"MINI-BATCH GRADIENT DESCENT THETA\",theta_mbgd)\n",
    "\n",
    "def plot_gradient_descent(X, y, theta_bgd, theta_sgd, theta_mbgd):\n",
    "  plt.plot(X, y, \"b.\")\n",
    "  X_new = np.array([[0], [2]])\n",
    "  X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "  print(X_new)\n",
    "  y_predict_bgd = X_new_b.dot(theta_bgd)\n",
    "  y_predict_sgd = X_new_b.dot(theta_sgd)\n",
    "  y_predict_mbgd = X_new_b.dot(theta_mbgd)\n",
    "  plt.plot(X_new, y_predict_bgd, \"r-\", linewidth = 4, label = \"BGD\")\n",
    "  plt.plot(X_new, y_predict_sgd, \"g-\", linewidth = 2, label = \"SGD\")\n",
    "  plt.plot(X_new, y_predict_mbgd, \"y-\", linewidth = 2, label = \"MBGD\")\n",
    "  plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "  plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "  plt.legend(loc=\"upper left\", fontsize=16)\n",
    "  plt.title(\"Gradient Descent\", fontsize=16)\n",
    "  plt.show()\n",
    "\n",
    "plot_gradient_descent(X, y, theta_bgd, theta_sgd, theta_mbgd)\n",
    "\n",
    "# Advanced Optimizers\n",
    "# Momentum-based Gradient Descent\n",
    "\n",
    "def gradient_descent_with_momentum(X, y, theta, alpha, gamma, num_iters):\n",
    "  m = len(y)\n",
    "  velocity = np.zeros_like(theta)\n",
    "\n",
    "  for iteration in range(num_iters):\n",
    "    gradient = 1/m * X.T.dot(X.dot(theta) - y)\n",
    "    velocity = gamma * velocity + alpha * gradient\n",
    "    theta = theta - velocity\n",
    "  return theta\n",
    "\n",
    "#Example usage\n",
    "X = np.array([[1,2],[3,4],[5,6]])\n",
    "y = np.array([1,2,3])\n",
    "theta = np.zeros(X.shape[1])\n",
    "alpha = 0.01\n",
    "gamma = 0.9\n",
    "num_iters = 1000\n",
    "\n",
    "theta_momentum = gradient_descent_with_momentum(X, y, theta, alpha, gamma, num_iters)\n",
    "print(\"Optimized paramters:\", theta_momentum)\n",
    "\n",
    "\n",
    "# Adagrad\n",
    "\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "alpha = 0.1\n",
    "n_epochs = 1000\n",
    "epsilon = 1e-8\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "gradient_accum = np.zeros((2,1))\n",
    "\n",
    "for iteration in range(n_epochs):\n",
    "  gradient = 2/len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "  gradient_accum += gradient**2\n",
    "  adjusted_gradient = gradient / (np.sqrt(gradient_accum) + epsilon)\n",
    "  theta = theta - alpha * adjusted_gradient\n",
    "\n",
    "print(\"Optimized paramters:\", theta)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, X_b.dot(theta), \"r-\", linewidth = 2, label = 'Adagrad')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "# RMSprop\n",
    "\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "alpha = 0.1\n",
    "n_epochs = 1000\n",
    "epsilon = 1e-8\n",
    "gamma = 0.9\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "gradient_accum = np.zeros((2,1))\n",
    "\n",
    "for i in range(n_epochs):\n",
    "  gradient = 2/len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "  gradient_accum = gamma*gradient_accum + (1-gamma)*(gradient**2)\n",
    "  adjusted_gradient = gradient / np.sqrt(gradient_accum + epsilon)\n",
    "  theta = theta - alpha * adjusted_gradient\n",
    "\n",
    "print(\"Optimized paramters:\", theta)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, X_b.dot(theta), \"r-\", linewidth = 2, label = 'Adagrad')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5024c37f-b34a-4d8e-8b17-31b8db8b370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OPTIMIZERS USING MNIST DATA\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape((60000, 28, 28, 1))  # Reshape for CNN\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))\n",
    "X_train = X_train.astype('float32') / 255  # Normalize to [0, 1]\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Create a simple CNN model\n",
    "def create_model(optimizer):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Optimizers\n",
    "optimizers = {\n",
    "    'Batch Gradient Descent': 'sgd',  # Using SGD as a representation\n",
    "    'Stochastic Gradient Descent': tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'Momentum': tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'Adagrad': tf.keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "}\n",
    "\n",
    "# Train and evaluate models using different optimizers\n",
    "results = {}\n",
    "for name, optimizer in optimizers.items():\n",
    "    model = create_model(optimizer)\n",
    "    print(f\"\\nTraining with {name}...\")\n",
    "    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=2)\n",
    "    results[name] = history.history\n",
    "\n",
    "# Plot accuracy for each optimizer\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, history in results.items():\n",
    "    plt.plot(history['val_accuracy'], label=name)\n",
    "    \n",
    "plt.title('Model Accuracy with Different Optimizers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss for each optimizer\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, history in results.items():\n",
    "    plt.plot(history['val_loss'], label=name)\n",
    "\n",
    "plt.title('Model Loss with Different Optimizers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
